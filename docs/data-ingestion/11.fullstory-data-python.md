---
sidebar_position: 11
---

# How to Ingest FullStory Data Export Extracts with Python

If you are interested in user tracking on your website, FullStory is a pretty good option. You can sign up for the free version here. The free version includes heaps of cool features. When you first sign up, you can try all the Pro Edition features for 2 weeks, too.

From data analytics perspective, FullStory gives us the user behaviour data as it collects clicks on the html elements. Analysing data will give us more insights into customer experience on your website and more. The first step is to ingest the data from it.

FullStory has a simple REST API to extract the click data. First, you retrieve a list with data export ids after a target date. By using the id, you can extract the actual click data. Once you make the GET request, the data comes in the gz format. So, all you have to do it to write it to a gz file from the results of the API request.

In this example, we are loading JSON data into Postgres database. You need to have Postgres table with one column with the jsonb data type as the target table.

```sql
CREATE TABLE fullstory.data_export (
    data_column jsonb
);
```

Code

The code is written in Python 3.

(1) Required modules

First of all, letâ€™s import all the required modules.

```python
import requests
from time import strptime, mktime
import gzip
import json
import psycopg2
import sys
```

(2) Set required variables

API credential needs to be created (see the instruction here). We are going to set variables for Postgres connections.

```python
api_token = 'api_token'
dbname = 'dbname'
user = 'user'
host = 'host url'
pw = 'password'
connection_string = "dbname='{}' user='{}' host='{}' password='{}'"\
.format(dbname, user, host, pw)
```

(3) Converting local date to UNIX Epoch

This function converts local date to the epoch timestamp format. For further timestamp conversion into epoch with Python, you can check out the reference here.

```python
def datetime_converter(datetime_string):
    '''
    The function should be used when both server and input datatime string
    is in the local time.
    '''
    # (1) Convert to datetime format
    target_timestamp = strptime(datetime_string, '%Y-%m-%d %H:%M:%S')
    # (2) mktime creates epoch time from the local time
    mktime_epoch = mktime(target_timestamp)
    print(int(mktime_epoch)) # convert to integer to remove decimal
    return int(mktime_epoch)
```

(4) Obtain the list of Data Export.

You first need to get the list of Data Export after the target date.

```python
def get_list(target_date):
    '''This function obtain List of data export
    from the target date onwards.'''
    output = None
    headers = {'Authorization':'Basic {}'.format(api_token)}
    uri = 'https://export.fullstory.com/api/v1/export/list?start={}'\
    .format(datetime_converter('2018-03-29 00:00:00'))
    # Call get list API
    r = requests.get(uri, headers=headers)
    status_code = r.status_code
    # Check status code
    if status_code == 200:
        print('List API call has been successful.')
        output = r.json()
        # print(output)
    else:
        print('List API call unsuccessful with status code:\
         {}'.format(status_code))

    return output
```

(5) Get Data Export

The function below uses the Data Export Id to retrieve the data and then creates a gz file. Once the gz file is created, the create_file() function creates a JSON file that is ready for database upload. I am adding the list id to all the records. This may help for troubleshooting or testing.

```python
def get_data_export(id, file_path):
    '''Get data export with the specified Id'''
    headers = {'Authorization':'Basic {}'.format(api_token)}
    uri = 'https://export.fullstory.com/api/v1/export/get?id={}'.format(str(id))
    r = requests.get(uri, headers=headers)
    status_code = r.status_code
    if status_code == 200:
        print('Returned header from API call: ')
        print(r.headers)

        with gzip.open(file_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=1024):
                f.write(chunk)
        print('gz file has been created as {}'.format(file_path))
    else:
        print('Get DataExport Api Call unsuccessful with status code:\
         {}'.format(status_code))

def create_file(input_path, output_path, list_id):
    '''Create Json & Csv File from Input File'''
    f =  gzip.open('/tmp/DataExport.json.gz', 'r')
    json_out = json.load(f)
    print('Check the Json content: \n' + str(json_out[0]))
    # for Python 3. Unless encoding, get UnicodeEncodeError
    output = open(output_path, 'w', encoding='utf8')
    for line in json_out:
        line['list_id'] = list_id
        output.write(json.dumps(line))
        output.write('\n')
    print('Completed File Creation')
```

(6) Upload JSON file to Postgres

The function simply copies the JSON file to Postgres.

```python
def pg_load(connection_string, table_name, file_path):
    '''
    This function load a flat file to PG Table with Jsonb data type.
    '''
    try:
        conn = psycopg2.connect(connection_string)
        print("Connecting to Database")
        cur = conn.cursor()
        f = open(file_path, "r")
        cur.copy_expert("copy {} FROM STDIN WITH CSV quote e'\x01' delimiter e'\x02'".format(table_name), f)
        cur.execute("commit;")
        print("Loaded data into {}".format(table_name))
        conn.close()
        print("DB connection closed.")

    except Exception as e:
        print('Error {}'.format(str(e)))
```

(7) Put it all together

The get_list() function retrieves a list of Data Exports. In the main function, we will loop the list.

```python
def main(start_date, target_table):
   # (1) Get response from the target_date
   list_response = get_list(start_date)
   for i in list_response['exports']:
       id = i['Id']
       print('Target Id is {}'.format(id))
       # (2) Create gz file from get api
       gz_file_path = '/tmp/fullstory/DataExport.json.gz'
       get_data_export(id, gz_file_path)
       # (3) Convert gz file into json file for upload
       json_file_path = '/tmp/fullstory/data_export_{}.json'.format(id)
       create_file(gz_file_path, json_file_path, id)
       # (4) Upload to PG
       pg_load(connection_string, target_table, json_file_path)

# Execution
start_date = '2018-03-29 00:00:00'
target_table = 'fullstory.data_export'
main(start_date, target_table)
Next Step
```

You can make the ingestion pattern as incremental. For example, you can write the list to the database and retrieve the max stop time and pass it as the target date. A little bit of customisation to the code above will enable you to do incremental ingestion. See how you go!

(2018-04-01)
